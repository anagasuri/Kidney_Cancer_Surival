# Kidney_Cancer_Surival
Survival Prediction of Kidney Renal Clear Cell Carcinoma using Clinical and Molecular Data

Survival Prediction Accuracy of Patients with Kidney Renal Clear Cell Carcinoma using Clinical Data vs. Molecular Data: Exploration of Data Trends using Various Machine Learning Models
Background:
I investigated the Kidney Renal Clear Cell Carcinoma dataset. When browsing the
dataset, I saw that there were 2 parts to the data, clinical and molecular. The clinical data had information regarding age, gender, cancer stage, and tumor grade. The molecular data was focused on hundreds of genes which seem to govern many pathways which lead to a functioning kidney. Irregular expression levels of these genes would lead to Kidney Renal Clear Cell Carcinoma. The key question that I investigated was whether there is a significant difference in predicting survival accuracy using clinical data versus molecular data. To explore this difference, I have tested each dataset on 5 different machine learning models. I decided to use a combination of supervised and unsupervised learning models including, Neural Networks, K-Nearest Neighbors, Support Vector Machines, Random Forest, and Logistic Regression.

<img width="856" alt="Screenshot 2025-01-09 at 7 07 00 PM" src="https://github.com/user-attachments/assets/9f28d28f-87bc-4e4d-bb80-26361e361874" />

Methods:
The target variable I used was OS_vital_status. It is obvious that some of the models will
perform better for certain datasets over others. For example, the clinical data subset I made has few features and they are also mostly categorical. They include age, gender, cancer stage, and tumor grade. The only numeric feature being “age”. These were also all the predictor variables in the clinical data. Since this dataset is smaller and less varied, it is expected that a model like a Random Forest would not be able to make as many branches since there is a lack of data. This means the model is more prone to overfitting and therefore incorrectly representing the data. Similarly, a Neural Network would not perform well with a smaller dataset either, as it requires larger datasets. However, models such as Logistic Regression, Support Vector Machine, and K- Nearest Neighbors can work better with smaller datasets. For the Support Vector Machine, I decided to use a linear kernel because of the size and low complexity of the clinical dataset. These three have the best representation of the clinical data subset. Prior to looking at predictive

accuracy, because the dataset is smaller, I made sure to use a 10-fold cross-validation when training the models. This can be seen in the “Data Sampler” tiles.
The molecular dataset is a much larger more varied. The feature included genes such as miRNA_hsa-mir-523. Because of the large number of genes, I did my best to remove as many categorical based genes as possible to feed the models a uniform body of data points. The numerical genes that I was left with were my predictor variables. Because of the complexity of the data, a model that can handle complex non-linear relationship data would be the best to represent it. As seen in the results, the Random Forest and Neural Network models had the highest predictive accuracies. This would make sense since the molecular dataset has hundreds of features. The Random Forest and Neural Networks require a large amount of data to not overtrain. Conversely, when exploring the Support Vector Machine’s performance on the molecular dataset, I switched between using the linear and RBF kernels. In theory, the RBF kernel should have been the better fit for this dataset since it is best for highly dimensional datasets. But I found that there wasn’t a huge difference between using the linear versus RBF kernels. To train these models, I still used 10-fold cross-validation just to keep the training method similar, and therefore comparison similar, to the clinical dataset. There was not much of a difference between using cross-validation or just a 70-30 split in the data. This can, again, be seen in the “Data Sampler” tiles.

<img width="603" alt="Screenshot 2025-01-09 at 7 07 32 PM" src="https://github.com/user-attachments/assets/959bb281-9199-4a14-ba01-420acab53116" />


Although the Neural Network performed the best on the clinical test data, knowing that the dataset is small, we can assume that since this is a complex model, that it was overfit. The Logistic Regression model performed the best, second to the Neural Network, which makes more sense as it is a simpler model.
As expected, the Neural Network and Random Forest models performed the best for the molecular data. Since the molecular data was incredibly complex with hundreds of genes as features, it needed a more complex model to capture the variance in the data. Therefore, it is expected that these two models would represent this data well.

<img width="489" alt="Screenshot 2025-01-09 at 7 08 05 PM" src="https://github.com/user-attachments/assets/93a03ac0-3a84-4684-ac4c-369e0fcc02d8" />

The distribution of stages is varied across the two survival statuses. The chi squared value is also relatively high at 54.59. This shows that there is a significant association between the stage variable and survival status. Because of the chi squared value, we can assume that this difference is unlikely to have occurred by random chance.

<img width="546" alt="Screenshot 2025-01-09 at 7 09 30 PM" src="https://github.com/user-attachments/assets/2a3c07c6-5cb7-4a4a-bceb-9172e099b634" />

This scatter plot shows that there may be a relationship between these two genes. For those who did not survive (category 1), there is higher expression levels of these genes observed together. In the upper right corner, that red cluster may show that there is a subgroup where these two genes are co-expressed at higher levels. This could indicate that they could later be a part of the same altered gene pathway that led to this cancer. The cluster of blue points closer to the origin from category 0, those who did survive, could show that both genes are usually not supposed to be highly expressed, which is why these patients survived.
 
Discussion:
For this project, I decided to divide the dataset into 2 subsets, clinical and molecular. My
original key question I investigated in this project was whether there is a significant difference in predicting survival accuracy using clinical data versus molecular data. I think my initial question is phrased incorrectly. We cannot predict survival purely based on clinical or molecular data. You need both subsets of the data to accurately predict patient survival. To rephrase, the key question should be “which predictive modeling method is most useful to predict patient survival using both clinical and molecular data?”.
I looked at 5 different supervised and unsupervised learning models to evaluate my data. I looked at Neural Networks, K-Nearest Neighbors, Support Vector Machines, Random Forest, and Logistic Regression. I ran 5 for the clinical data and 5 for the molecular data. My main factor of accuracy was looking at the AUC values and Confusion Matrices of these models based on each subset of the data.
As discussed in the results, although Neural Networks seemed to take the lead on predicating outcomes in the clinical dataset, the model is too complex for this smaller and mainly categorical dataset. Therefore, we can assume that it overfit the data, even though we used 10-fold cross- validation. Consequently, Logistic Regression, which was the next best performing model with an AUC value of 0.833 was the best fit for the clinical data. Once again, due to the data’s fewer features, a simpler model was expected to perform better. According to its confusion matrix, it predicted 19/25 instances accurately.
The best performing models for the molecular dataset were Random Forest and Neural Networks. This was not surprising seeing as this dataset was extremely complex with many features. Therefore, more complex models would perform the best. The AUC values of the Random Forest and Neural Network were both 0.875. According to their confusion matrices, they both predicted 11/13 instances accurately.
One of the limitations to analyzing this data is there are more males (70) than females (50) in this cohort, and the number of people in each age group varies (<50 years: 30, 50-60 years: 50, >50 years 40). It would be hard to compare if age or gender is a large factor in getting this cancer because there isn’t a valid way to standardize these variables without imputing incorrect data or having to analyze a dataset with too many missing values. Another limitation to the analysis is that some of the models I chose were too complex for the dataset they were being given. As mentioned before, the clinical data subset is limited with few variables, so using models like Neural Networks or Random Forests overfit the data. These models are not appropriate for this kind of data. On the other hand, for the molecular data subset, it was clear that this data was very complex with hundreds of features. Therefore, using models like the Logistic Regression or Support Vector machine meant they were underfit. Both these scenarios lead to biased outcomes because the models cannot accurately value the important nuances of the data. Finally, the last source of bias could be that I did not completely understand which variables were necessary or not. Since this was a very complicated dataset, an expert may be able to weed out some of the genes which would not be as impactful, and therefore decreasing the dimensionality of the data. But to a user who does not have a deep background in biology, this would be a significant hurdle.

Final Workflow:

 <img width="477" alt="Screenshot 2025-01-09 at 7 09 46 PM" src="https://github.com/user-attachments/assets/8c894915-7aef-479a-a219-0f274ac92e99" />
